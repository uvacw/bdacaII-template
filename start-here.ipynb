{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a720ba10",
   "metadata": {},
   "source": [
    "# Setup & Environment Check Notebook\n",
    "\n",
    "This notebook verifies that all packages in the course environment are installed\n",
    "and working correctly ‚Äî **without errors or warnings**.\n",
    "\n",
    "It also demonstrates how to use the project folder structure:\n",
    "\n",
    "- **data/raw/** ‚Äì Original data\n",
    "- **data/processed/** ‚Äì Cleaned data for analysis\n",
    "- **tables/** ‚Äì Final visualizations and outputs\n",
    "\n",
    "After running this notebook, check these folders to see example outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6935ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Environment setup and package version check\n",
    "\n",
    "import sys, platform, warnings\n",
    "from importlib.metadata import version as dist_version, PackageNotFoundError\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up project directories\n",
    "RAW_DATA = Path(\"data/raw\")\n",
    "PROCESSED_DATA = Path(\"data/processed\")\n",
    "TABLES = Path(\"tables\")\n",
    "\n",
    "for folder in [RAW_DATA, PROCESSED_DATA, TABLES]:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project structure:\")\n",
    "print(f\"  Raw data:       {RAW_DATA.resolve()}\")\n",
    "print(f\"  Processed data: {PROCESSED_DATA.resolve()}\")\n",
    "print(f\"  Tables/plots:   {TABLES.resolve()}\")\n",
    "\n",
    "print(\"\\nPython:\", platform.python_version())\n",
    "print(\"Executable:\", sys.executable)\n",
    "\n",
    "# Map module import names -> PyPI names\n",
    "DIST_FOR_MODULE = {\n",
    "    \"bs4\": \"beautifulsoup4\",\n",
    "    \"docx\": \"python-docx\",\n",
    "    \"PIL\": \"pillow\",\n",
    "    \"sklearn\": \"scikit-learn\",\n",
    "}\n",
    "\n",
    "def version_of(mod_name: str) -> str:\n",
    "    \"\"\"Return installed version or friendly message.\"\"\"\n",
    "    try:\n",
    "        m = __import__(mod_name)\n",
    "    except Exception:\n",
    "        return \"not importable\"\n",
    "    v = getattr(m, \"__version__\", None)\n",
    "    if v:\n",
    "        return v\n",
    "    dist_name = DIST_FOR_MODULE.get(mod_name, mod_name)\n",
    "    try:\n",
    "        return dist_version(dist_name)\n",
    "    except PackageNotFoundError:\n",
    "        return \"installed (no version metadata)\"\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "# Core packages from pyproject.toml\n",
    "modules = [\n",
    "    \"pandas\", \"numpy\", \"scipy\", \"openpyxl\", \"pyarrow\",\n",
    "    \"matplotlib\", \"seaborn\", \"wordcloud\",\n",
    "    \"wikipediaapi\", \"mwparserfromhell\", \"wikitextparser\", \"mwclient\",\n",
    "    \"bs4\", \"lxml\", \"regex\", \"unidecode\", \"nltk\", \"textblob\", \"textstat\", \"dateparser\",\n",
    "    \"statsmodels\", \"sklearn\", \n",
    "    \"transformers\", \"torch\", \"sentence_transformers\", \"gensim\",\n",
    "    \"networkx\", \"PIL\", \"requests\", \"httpx\",\n",
    "    \"docx\", \"pdfplumber\", \"tqdm\", \"nbformat\"\n",
    "]\n",
    "\n",
    "print(\"\\nPackage versions:\")\n",
    "for m in modules:\n",
    "    print(f\"  {m:<20} {version_of(m)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dacd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Demo dataset (simulating Wikipedia article analysis)\n",
    "\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "csv_text = \"\"\"title,views,length,links,categories,date_edited\n",
    "Python (programming language),1250400,82000,450,12,2025-01-15\n",
    "Machine learning,890200,65000,380,15,2025-01-20\n",
    "Natural language processing,420300,48000,290,10,2025-02-01\n",
    "Data science,670500,55000,320,11,2025-02-10\n",
    "Artificial intelligence,1580600,95000,520,18,2025-03-05\n",
    "\"\"\"\n",
    "\n",
    "raw_path = RAW_DATA / \"wikipedia_articles_raw.csv\"\n",
    "with open(raw_path, 'w') as f:\n",
    "    f.write(csv_text)\n",
    "print(f\"‚úì Saved raw data -> {raw_path}\")\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_text))\n",
    "df[\"date_edited\"] = pd.to_datetime(df[\"date_edited\"])\n",
    "df[\"links_per_1000_chars\"] = (df[\"links\"] / df[\"length\"]) * 1000\n",
    "df[\"categories_per_1000_chars\"] = (df[\"categories\"] / df[\"length\"]) * 1000\n",
    "\n",
    "processed_path = PROCESSED_DATA / \"wikipedia_articles_cleaned.csv\"\n",
    "df.to_csv(processed_path, index=False)\n",
    "print(f\"‚úì Saved processed data -> {processed_path}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9eb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Text processing demo (BeautifulSoup, regex, unidecode)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re\n",
    "from unidecode import unidecode\n",
    "\n",
    "html = \"\"\"\n",
    "<article>\n",
    "  <h1>Caf√© in Z√ºrich</h1>\n",
    "  <p>A story about <em>data journalism</em> in 2025.</p>\n",
    "</article>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "title = soup.h1.text\n",
    "text = soup.p.text\n",
    "\n",
    "title_ascii = unidecode(title)\n",
    "text_clean = re.sub(r'\\d+', '[NUM]', text)\n",
    "\n",
    "print(\"‚úì Text processing working\")\n",
    "print(f\"  Original title: {title}\")\n",
    "print(f\"  ASCII title:    {title_ascii}\")\n",
    "print(f\"  Cleaned text:   {text_clean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca06060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) NLP basics (NLTK, TextBlob, TextStat)\n",
    "\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "\n",
    "# Download ALL required NLTK data\n",
    "print(\"Downloading NLTK data (this may take a moment)...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('universal_tagset', quiet=True)\n",
    "\n",
    "sample_text = \"Natural language processing is a fascinating field. It combines linguistics and computer science.\"\n",
    "\n",
    "# Just use simple word operations, avoid sentiment which needs corpora\n",
    "blob = TextBlob(sample_text)\n",
    "word_count = len(blob.words)\n",
    "\n",
    "flesch_score = textstat.flesch_reading_ease(sample_text)\n",
    "syllable_count = textstat.syllable_count(sample_text)\n",
    "\n",
    "print(\"‚úì NLP packages working\")\n",
    "print(f\"  Words: {word_count}\")\n",
    "print(f\"  Syllables: {syllable_count}\")\n",
    "print(f\"  Flesch reading ease: {round(flesch_score, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfebbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Basic visualization (Matplotlib & Seaborn)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(data=df, x=\"length\", y=\"views\", size=\"links\", hue=\"categories\", sizes=(50, 300))\n",
    "plt.title(\"Wikipedia Article Performance\")\n",
    "plt.xlabel(\"Article Length (characters)\")\n",
    "plt.ylabel(\"Views\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = TABLES / \"article_performance.png\"\n",
    "plt.savefig(plot_path, dpi=150)\n",
    "print(f\"‚úì Saved plot -> {plot_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4f6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Word cloud visualization\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "text = \" \".join(df[\"title\"])\n",
    "wc = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Wikipedia Article Topics\")\n",
    "plt.tight_layout()\n",
    "\n",
    "wc_path = TABLES / \"wordcloud.png\"\n",
    "plt.savefig(wc_path, dpi=150)\n",
    "plt.close()\n",
    "print(f\"‚úì Saved wordcloud -> {wc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9173a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Statistical analysis (Statsmodels)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(df[\"length\"])\n",
    "model = sm.OLS(df[\"views\"], X).fit()\n",
    "\n",
    "print(\"‚úì Statistical packages working\")\n",
    "print(f\"  OLS R¬≤: {round(model.rsquared, 3)}\")\n",
    "print(f\"  p-value for length: {round(model.pvalues[1], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4746f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Machine Learning basics (scikit-learn)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "features = df[[\"length\", \"links\", \"categories\"]].values\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = kmeans.fit_predict(features_scaled)\n",
    "\n",
    "print(\"‚úì Machine learning packages working\")\n",
    "print(f\"  Cluster distribution:\\n{df['cluster'].value_counts()}\")\n",
    "\n",
    "clustered_path = PROCESSED_DATA / \"articles_with_clusters.csv\"\n",
    "df.to_csv(clustered_path, index=False)\n",
    "print(f\"‚úì Saved clustered data -> {clustered_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb25f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Network analysis demo (NetworkX)\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([\n",
    "    (\"Python\", \"Data Science\"),\n",
    "    (\"Python\", \"Machine Learning\"),\n",
    "    (\"Data Science\", \"Machine Learning\"),\n",
    "    (\"Machine Learning\", \"AI\"),\n",
    "    (\"NLP\", \"AI\")\n",
    "])\n",
    "\n",
    "print(\"‚úì Network analysis packages working\")\n",
    "print(f\"  Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Density: {round(nx.density(G), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f06716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Web requests demo (requests, httpx)\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://api.github.com/zen\")\n",
    "if response.status_code == 200:\n",
    "    print(\"‚úì Web request packages working\")\n",
    "    print(f\"  GitHub Zen: {response.text[:50]}...\")\n",
    "else:\n",
    "    print(\"‚úì Requests installed (couldn't reach API)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e591db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Document export (python-docx)\n",
    "\n",
    "from docx import Document\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading(\"Setup Check Report\", level=1)\n",
    "doc.add_paragraph(f\"All {len(modules)} required packages are installed and working correctly.\")\n",
    "doc.add_heading(\"Analysis Summary\", level=2)\n",
    "doc.add_paragraph(f\"Analyzed {len(df)} Wikipedia articles\")\n",
    "doc.add_paragraph(f\"Average views: {df['views'].mean():,.0f}\")\n",
    "doc.add_paragraph(f\"Average length: {df['length'].mean():,.0f} characters\")\n",
    "\n",
    "doc_path = TABLES / \"setup_report.docx\"\n",
    "doc.save(doc_path)\n",
    "print(f\"‚úì Saved Word document -> {doc_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b550c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Word embeddings and transformers demo\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading small sentence embedding model (this may take 30 seconds)...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Tiny, fast model\n",
    "\n",
    "sentences = [\n",
    "    \"Natural language processing is fascinating\",\n",
    "    \"Machine learning is powerful\",\n",
    "    \"Data science is useful\",\n",
    "    \"The weather is nice today\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Calculate similarity between first sentence and others\n",
    "similarities = []\n",
    "for i in range(1, len(embeddings)):\n",
    "    sim = np.dot(embeddings[0], embeddings[i]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[i]))\n",
    "    similarities.append(sim)\n",
    "\n",
    "print(\"‚úì Sentence transformers working\")\n",
    "print(f\"  Model: all-MiniLM-L6-v2 (384-dim embeddings)\")\n",
    "print(f\"  Embedding shape: {embeddings[0].shape}\")\n",
    "print(f\"\\n  Similarity to '{sentences[0]}':\")\n",
    "for i, (sent, sim) in enumerate(zip(sentences[1:], similarities), 1):\n",
    "    print(f\"    - '{sent}': {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b65ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ All checks completed successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(f\"  üìÅ {RAW_DATA}/       - 1 raw data file\")\n",
    "print(f\"  üìÅ {PROCESSED_DATA}/ - 2 processed data files\")\n",
    "print(f\"  üìÅ {TABLES}/         - 4 output files\")\n",
    "print(\"\\nCore capabilities verified:\")\n",
    "print(\"  ‚úì Data manipulation (pandas, numpy)\")\n",
    "print(\"  ‚úì Text processing (BeautifulSoup, regex, NLTK)\")\n",
    "print(\"  ‚úì NLP analysis (TextBlob, TextStat)\")\n",
    "print(\"  ‚úì Machine learning (scikit-learn)\")\n",
    "print(\"  ‚úì Word embeddings (sentence-transformers)\")\n",
    "print(\"  ‚úì Network analysis (NetworkX)\")\n",
    "print(\"  ‚úì Web APIs (requests)\")\n",
    "print(\"  ‚úì Visualization (matplotlib, seaborn, wordcloud)\")\n",
    "print(\"\\nYou're ready to start analyzing Wikipedia data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) OPTIONAL: Clean up test data\n",
    "\n",
    "# To use this cleanup code:\n",
    "# 1. Select all the lines below (starting from \"import shutil\" to the last print statement)\n",
    "# 2. Press Ctrl+/ (Windows/Linux) or Cmd+/ (Mac) to uncomment all selected lines\n",
    "# 3. Run the cell\n",
    "#\n",
    "# This will remove all generated test files and return the repository to its original state.\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# print(\"Cleaning up test data...\")\n",
    "\n",
    "# # Remove all files from data/raw, data/processed, and tables\n",
    "# for folder in [RAW_DATA, PROCESSED_DATA, TABLES]:\n",
    "#     if folder.exists():\n",
    "#         for file in folder.glob(\"*\"):\n",
    "#             if file.is_file():\n",
    "#                 file.unlink()\n",
    "#                 print(f\"  Deleted {file.name}\")\n",
    "\n",
    "# print(\"\\n‚úì Repository cleaned! Folders are now empty.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
